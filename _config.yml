repository: sproogen/resume-theme


favicon: images/favicon.ico


version: 2


name: Rohan Rathore
title: Data Engineer | Bengaluru, India
phone_title: Contact 
phone: rohan.rathore93@gmail.com
email_title: Status
email: Verified Expert


darkmode: never


twitter_username: rohanredtj
github_username:  rohanredtj
linkedin_username: rohanredtj
instagram_username: rohanredtj


additional_links:
- title: Hire Rohan
  icon: fas fa-briefcase
  url: https://www.upwork.com/freelancers/~01ee871ca78c1a805c


about_profile_image: images/profile.jpg
about_content: |

  ## **Expertise**
  <blockquote><span class="block-main"><svg width="12" height="12" viewBox="0 0 14 14" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="m7 10.5-4.114 2.163.785-4.581L.343 4.837l4.6-.669L7 0l2.057 4.168 4.6.669-3.328 3.245.785 4.581z"></path></svg>Data Engineering</span> <span class="block-secondary">Data Analysis</span> <span class="block-secondary">Data Warehousing</span> <span class="block-secondary">ETL</span> <span class="block-secondary">Big Data</span> <span class="block-secondary">Cloud Platforms</span> <span class="block-secondary">Data Modeling</span> <span class="block-secondary">Database Design</span> <span class="block-secondary">Data Pipeline Architecture</span> <span class="block-secondary">Version Control</span> <span class="block-secondary">Containerization</span></blockquote>
  
  ## **Portfolio Projects**
  [Data Engineering](#data-engineering-projects), [Data Science](#data-science-projects), [Data Analytics](#data-analytics-projects), [Software Engineering](#software-engineering-projects), [Other](#other-projects)
  <br><br> 

  ## **Bio**
  Rohan is a data professional with 7 years of experience, two master's degrees, and a proven track record of working with US-based companies. His expertise spans the entire data lifecycle, from data engineering and analytics to visualization and machine learning, allowing him to quickly adapt to new technologies and methodologies. Rohan's international exposure, diverse skill set, and eagerness to learn make him well-equipped to tackle complex data challenges and drive impactful solutions in fast-paced environments.
  <br><br> 

  ## **Experience**
  <blockquote><span class="block-secondary">Python - 6 years</span> <span class="block-secondary">Predictive Modeling - 6 years</span> <span class="block-secondary">SQL - 5 years</span> <span class="block-secondary">BigQuery - 3 years</span> <span class="block-secondary">Data Modelling - 3 years</span> <span class="block-secondary">Keras - 3 years</span> <span class="block-secondary">Django - 2 years</span> <span class="block-secondary">Amazon Web Services (AWS) - 2 years</span> <span class="block-secondary">PySpark - 2 years</span> <span class="block-secondary">DBT - 2 years</span> <span class="block-secondary">Docker - 1 years</span></blockquote>

  ## **Education**
  - Master of Science in Business Consulting, Furtwangen University, Germany ([Master Thesis](https://drive.google.com/file/d/1b4AR98QbptOp3krVO1BYh-TDCWeSl1U1/view?usp=drive_link))
  - Master of Science in Big Data Analytics & AI, Novosibirsk State University, Russia ([Master Thesis](https://link.springer.com/chapter/10.1007/978-981-16-5747-4_73))
  - Bachelor of Technology in Engineering Physics, Delhi Technological University, India
  <br><br> 

  ## **Certifications**
  - Dell EMC Data Science Assiociate ([NCB172QTLEFQQG55](https://www.certmetrics.com/dell/public/verification.aspx))
  <br><br> 

  ## **Skills**
  - **Libraries/APIs** - Pandas, NumPy, Scikit-learn, Apache Spark, Django
  - **Tools** - DBT, Docker, Apache Airflow
  - **Languages** - Python, SQL, R
  - **Paradigms** - ETL/ELT, Data Modeling, Data Warehousing, Big Data Processing
  - **Platforms** - MySQL, PostgreSQL, Amazon Web Services (AWS), Google Cloud Platform (GCP)
  - **Storage** - Amazon Redshift, Google BigQuery, Snowflake
  - **Other** - Data Pipeline Design, Data Quality Management, Data Analytics, Machine Learning
  <br><br>  

  ## **Preferred Environment**
  Visual Studio Code (VS Code), Amazon Web Services (AWS), Google Colab, MacOS


content:

  - title: Data Engineering Projects
    layout: list
    content:
      - layout: left #pid-1
        title: AWS-Powered Zillow Data Pipeline for Real Estate Analytics
        link: foo
        link_text: Code Repository ðŸ”— 
        
        description: |     
          
          Engineered a data pipeline using medallion architecture for <mark>Zillow data</mark>, processing 1GB of daily scraped CSV files from AWS S3. Implemented <mark>data cleaning</mark> and standardization for 180 columns, storing results in Parquet format with Hive partitioning. Deployed as an AWS Lambda function with <mark>scheduled execution</mark> via CloudWatch, optimized for 3GB RAM allocation.
          
          **Technologies:** AWS S3, AWS Lambda, AWS CloudWatch, AWS CLI, Python, Pandas, PyArrow, CSV, Parquet, Hive partitioning

          ---

      - layout: left #pid-2
        title: Wiktionary Abbreviation Extractor for Legal Documents
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: |
          
          Developed a sophisticated tool to extract and analyze abbreviations from <mark>Wiktionary dumps</mark>. The project involved <mark>parsing</mark> complex data structures, implementing custom algorithms for <mark>abbreviation detection</mark>, and handling various <mark>edge cases</mark> to ensure comprehensive coverage of linguistic variations.
          
          **Technologies:** Python, XML processing libraries, regular expressions, data structures, version control (Git), shell scripting

          ---

      - layout: left #pid-3
        title: Automated Name Parsing for Healthcare Provider Lists
        link: foo
        link_text: Code Repository ðŸ”— 
        
        description: |     
          
          Engineered a sophisticated Python-based system utilizing OpenAI to automate the <mark>extraction and parsing</mark> of healthcare provider information from diverse state-specific files. The system adeptly handles multiple file formats including PDF, Excel, and CSV, processing data from over <mark>10 different states</mark>. It manages complex variations in data structure, ensuring clean and accurate output for <mark>internal compliance</mark> and monitoring use.
          
          **Technologies:** Python, OpenAI API, CSV handling libraries, Time management and logging tools

          ---

      - layout: left #pid-4
        title: Database Migration from Snowflake to PostgreSQL
        link: foo
        link_text: Code Repository ðŸ”— 
        
        description: |     
          
          Developed a robust Python script to <mark>automate</mark> the process of <mark>migrating data</mark> from Snowflake to PostgreSQL. This tool ensures <mark>efficient</mark> and error-free data transfer between disparate database systems. It handles schema creation, table synchronization, and large-scale data movement with <mark>built-in error handling</mark>. The script also generates timestamped schemas for version control and easy rollback if needed.
          
          **Technologies:** Python, SQLAlchemy, Pandas, Snowflake, PostgreSQL, SQL

          ---
      
      - layout: left #pid-5
        title: Anime Season Scraper Web Scraping Tool for MyAnimeList
        link: foo
        link_text: Code Repository ðŸ”— 
        
        description: |     
          
          This project developed a web <mark>scraping</mark> tool to collect seasonal <mark>anime data</mark> from MyAnimeList. The script uses the MyAnimeList API to fetch information about anime releases for specified years and seasons. It then processes and organizes this data into a <mark>structured format</mark> using Pandas, saving the results as a CSV file for further analysis or use in other applications.
          
          **Technologies:** Python, Pandas, Requests, MyAnimeList API

          ---

      - layout: left #pid-6
        title: PySpark-Based Oil Price Prediction Model
        link: foo
        link_text: Code Repository ðŸ”— 
        
        description: |     
          
          Developed a sophisticated <mark>machine learning model</mark> using PySpark to <mark>predict oil prices</mark> based on comprehensive oil field data. This project leveraged big data technologies to process and <mark>analyze</mark> large-scale datasets, enhancing decision-making capabilities in the energy sector. The model incorporated various predictive techniques, including Facebook Prophet, and was <mark>deployed</mark> on AWS infrastructure for scalable performance and real-time insights.
          
          **Technologies:** PySpark, AWS EC2 (r5.2xlarge instances), Jupyter Notebook, Facebook Prophet, Scikit-learn, Apache Superset, AWS S3, SSH, Python

          ---

      - layout: left #pid-7
        title: Web Scraping Forbes Russia Top 200 Private Companies
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: |
          
          Developed a Python script to <mark>extract</mark> and structure data from <mark>Forbes Russia's</mark> list of top 200 private companies. The project involved web scraping techniques to gather <mark>detailed information</mark> including company rankings, names, financial data, industry sectors, management details, and geographical information. The extracted data was then organized into a comprehensive <mark>dataset</mark>, enabling in-depth analysis of Russia's leading private businesses.
          
          **Technologies:** Python, BeautifulSoup, Pandas, NumPy, Requests, tqd
    
  - title: Data Science Projects
    layout: list
    content:
      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development

          ---

      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development

  - title: Data Analytics Projects
    layout: list
    content:
      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development

          ---

      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development
         
  - title: Software Engineering Projects
    layout: list
    content:
      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development

          ---

      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development
         
  - title: Other Projects
    layout: list
    content:
      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development

          ---

      - layout: left #pid-
        title: Real-time Stock Market Data Pipeline
        link: github.com/sproogen
        link_text: Project Website ðŸ”— 
        
        description: | # this will include new lines to allow paragraphs      
          
          This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
          **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development
         
  # - title: Test Projects
  #   layout: list
  #   content:
  #     - layout: left #pid-
  #       title: Real-time Stock Market Data Pipeline
  #       link: github.com/sproogen
  #       link_text: Project Website ðŸ”— 
        
  #       description: | # this will include new lines to allow paragraphs      
          
  #         This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
  #         **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development

  #         ---

  #     - layout: left #pid-
  #       title: Real-time Stock Market Data Pipeline
  #       link: github.com/sproogen
  #       link_text: Project Website ðŸ”— 
        
  #       description: | # this will include new lines to allow paragraphs      
          
  #         This project implements a real-time data pipeline that ingests stock market data, processes it, and provides actionable insights through an interactive dashboard. Using Apache Kafka for data streaming, Apache Spark for processing, and Tableau for visualization, the system handles high-volume financial data with low latency, enabling traders to make informed decisions quickly.
          
  #         **Technologies:** Informatica, BMC Remedy, Shell Scripting, Python, APIs, Java, Perl, SQL, ETL, ETL Development


footer_show_references: false


remote_theme: sproogen/resume-theme


sass:
  sass_dir: _sass
  style: compressed


plugins:
 - jekyll-seo-tag
